{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0490293f-10e3-4419-a2ff-8cfa024318ea",
   "metadata": {},
   "source": [
    "## Esame di Metodi Numerici\n",
    "\n",
    "21 Luglio 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c208635-191b-4d75-85a5-d3aa5d6ce31d",
   "metadata": {},
   "source": [
    "# Esercizio 1\n",
    "Nel file ``testI.mat`` sono memorizzati la matrice A ed il vettore b. \n",
    "\n",
    " - Verificare che per risolvere il sistema Ax=b sia possibile utilizzare i due metodi di discesa visti a lezione, **Punti 1**\n",
    " - implementare entrambi gli algoritmi e dire quante iterazioni sono necessarie per ciascuno di essi  per calcolare la soluzione con una toll=1e-6 e maxit=2000. Visualizzare in un grafico l'errore in scala logaritmica ad ogni iterazione per ciascuno dei due metodi. **Punti 6**\n",
    " - Verificare se la matrice è  malcondizionata, dire teoricamente cosa questo implica in termini della velocità di convergenza  dei due metodi alla soluzione, richiamando il risultato teorico visto a lezione.**Punti 2**\n",
    "\n",
    " - Costruire la matrice A1, ottenuta a partire da A, sommandole uma matrice diagonale con elementi sulla diagonali tutti uguali a 0.05, e termine noto b1, costruito in maniera tale che la soluzione del sistema A1 x1 =b1 sia il vettore formato da tutti 1. Risolvere il sistema lineare con matrice dei coefficienti A1 e termine noto b1, sia con il metodo del gradiente che con il metodo del gradiente coniugato. Osservare il numero di iterazioni eseguito da ciascun metodo e giustificare i risultati  **Punti 2**\n",
    "  \n",
    "   **Totale: Punti 11**\n",
    "\n",
    "Per la lettura dei dati procedere nel seguente modo:\n",
    "\n",
    "``from scipy.io import loadmat``\n",
    "\n",
    "``import numpy as np``\n",
    "\n",
    "``dati = loadmat('testI.mat')``\n",
    "\n",
    "``A=dati[\"A\"] ``\n",
    "\n",
    "``A=A.astype(float)``\n",
    "\n",
    "`` b=dati[\"b\"] ``\n",
    "\n",
    "`` b=b.astype(float)``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d5f387-410e-4cc2-a5b7-1644b968cab3",
   "metadata": {},
   "source": [
    "# Esercizio 2\n",
    "\n",
    "Siano assegnati i vettori\n",
    "$$\n",
    "x = [1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4, 2.6, 2.8, 3.0] , $$\n",
    "$$\n",
    "y = [1.18, 1.26, 1.23, 1.37, 1.37, 1.45, 1.42, 1.46, 1.53, 1.59, 1.59] \n",
    "$$\n",
    "\n",
    "contenenti rispettivamente le ascisse e le ordinate di 11 punti del piano.\n",
    "\n",
    "Scrivere lo script  in cui\n",
    " - si determini il polinomio di approssimazione ai minimi quadrati di grado 1 dei  punti assegnati sviluppando  le functions necessarie; **Punti: 4**\n",
    "- si determini il polinomio di interpolazione dei punti assegnati sviluppando le functions necessarie;  **Punti: 4**\n",
    "- si consideri il set di dati ($x_i,\\hat{y}_i$) , dove $\\hat{y}_i = 0.2 x_i+1$, $i = 0, ..., 10$ e si ricalcolino il polinomio di approssimazione ai minimi quadrati di grado 1 e il polinomio di interpolazione a partire dai nuovi punti $(x_i, \\hat{y}_i)$, i = 0, ..., 10; **Punti: 1**\n",
    "- si rappresentino in figura 1 (rispettivamente in figura 2) il primo (rispettivamente il secondo) set di punti assegnato insieme al polinomio di approssimazione ai minimi quadrati e al polinomio di interpolazione ottenuti da questi dati; **Punti: 1**\n",
    "- si fornisca una spiegazione teorica ai risultati ottenuti, utilizzando i teoremi visti a lezione.   **Punti: 4**\n",
    "\n",
    "**Totale:  14**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf4b3a3-57ab-4770-850c-08e78ee782ad",
   "metadata": {},
   "source": [
    "## Domanda intelligenza artificiale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17c2c65-5a19-4aff-8002-aacd16ab0f97",
   "metadata": {},
   "source": [
    "- Allenamento di una rete neurale: dopo aver descritto come è composta una rete neurale, descrivi in cosa consiste la fase di forward propagation e la fase di backward propagation. **Punti: 1**\n",
    "- Ottimizzazione della loss function per il training di una rete neurale per il task di regressione: Metodo di discesa del gradiente, metodo stocastico del gradiente, metodo del gradiente minibatch.  **Punti 1**  \n",
    " - Non convessità della loss-function - come non rimanere bloccati in un monimo locale? Metodo del gradiente con momentum. **Punti 2**\n",
    "- Learning rate scheduling: step decay, decadimento esponenziale, decadimento dipendente dal tempo. **Punti 1**\n",
    " - Learning rate adattivo: Adagrad, RMSProp, Adadelta, Adam. **Punti 2**\n",
    " \n",
    " **Totale:  7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33af564-b909-42b7-9367-73c63bc10faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca016394-d87b-4435-86e6-72d84aff851d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
